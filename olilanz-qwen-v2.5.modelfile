# Very promising! Few minor mis-steps, but able to recover and continue
# 56k was slow but stale. 48k is fast, but crashed ollama worker process.

# Prompt:
# I want you to improve maintainability of the Python code in @/inference/gradio_server.py . Organize the imports, and factor the args parsing out into a new file ultils/args.py. Add short documentation on every function to indicate the purpose of the function.

#FROM MHKetbi/Qwen2.5-Coder-32B-Instruct-Roo:q8_0
# 32GB VRAM at 56k context size.
# 51GB VRAM at 32k context size.

FROM hf.co/lmstudio-community/Qwen2.5-14B-Instruct-1M-GGUF:Q8_0
# Supports up to 1M context size.
# 40GB VRAM at 56k context size.
# PARAMETER num_ctx 57344
# 36GB VRAM at 48k context size. But crashes !?
# PARAMETER num_ctx 49125
# 34GB VRAM at 42k context size. But crashes !?
PARAMETER num_ctx 43008

#PARAMETER num_ctx 32768
#PARAMETER num_ctx 65536
