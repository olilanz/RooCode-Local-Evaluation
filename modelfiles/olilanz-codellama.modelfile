# No errors, but seems to run infinitely

#FROM codellama:13b-code

# With context size 32k, the model uses 50GB of VRAM
# With context size 16k, the model uses 32GB of VRAM
FROM codellama:13b-code-q8_0

# The model can only see 16k at once
# llama_model_loader: - kv   2:                       llama.context_length u32              = 16384

#FROM codellama:7b-code-q8_0

# print_info: n_ctx_train      = 16384
# print_info: rope type        = 0
# print_info: rope scaling     = linear

PARAMETER num_ctx 16384
#PARAMETER num_ctx 32768
#PARAMETER num_ctx 57344
#PARAMETER num_ctx 65536
