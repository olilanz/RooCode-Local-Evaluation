
# Good combo. 32GB VRAM, Reasonable refactoring.
# Seems to be hanging some times. 

FROM hf.co/tensorblock/deepseek-coder-6.7b-base-GGUF:Q8_0

# 50GB VRAM, at 56k context size

# llama.context_length	16,384	Base trained context length
# llama.rope.dimension_count	128	RoPE position encoding size
# llama.rope.scaling.type	linear	Uses linear RoPE scaling for extended context
# llama.rope.scaling.factor	4.0	Suggests it was extrapolated to 4× its trained length (16K × 4 = 64K)

# Likely good up to 32K tokens, potentially usable up to 64K tokens (loss of precision in long context)

# Beyond 64k, expect the following issues:
# - Loss of recall (forgetting early tokens).
# - Repetitive outputs.
# - Token drift (off-topic responses).


PARAMETER num_ctx 32768
#PARAMETER num_ctx 57344
#PARAMETER num_ctx 65536
#PARAMETER num_ctx 100000
#PARAMETER num_ctx 131072
